# -*- coding: utf-8 -*-
"""Copy of Tour_Guide_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UuErMXGO4TUhe-bm2i09tGX9zUlnDUZo
"""



"""### Data loaders"""

# Commented out IPython magic to ensure Python compatibility.
!pip install llama-index-readers-file llama-index-readers-web
!pip install unstructured
# %pip install llama-index
# %pip install transformers accelerate bitsandbytes
# %pip install llama-index-readers-web
# %pip install llama-index-llms-huggingface
# %pip install llama-index-embeddings-huggingface
# %pip install llama-index-program-openai
# %pip install llama-index-agent-openai
# %pip install -U bitsandbytes
!pip install -U sentence-transformers
!pip install llama-index
!pip install llama-index-embeddings-huggingface
!pip install chromadb llama-index-vector-stores-chroma pinecone-client llama-index-vector-stores-pinecone

from google.colab import drive
drive.mount('/content/drive')



import chromadb
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext

from pathlib import Path
from llama_index.readers.file import PDFReader

# loader=PDFReader()

# with open("/content/full_adder1.pdf", 'rb') as f: # Open the file in binary mode ('rb')
#   text=f.read()

# from llama_index.core import Document
#

# text

# documents=Document(text="text")

# documents=loader.load_data(file=Path('/content/full_adder1.pdf'))

from llama_index.core import SimpleDirectoryReader

reader = SimpleDirectoryReader(input_dir="/content/drive/MyDrive/verilog")
documents = reader.load_data()

from llama_index.core.node_parser import SentenceSplitter

splitter = SentenceSplitter(
    chunk_size=100,
    chunk_overlap=20,
)
nodes = splitter.get_nodes_from_documents(documents)
nodes[0].text

len(documents)

len(nodes)

from llama_index.core import VectorStoreIndex, Document

documents = [Document(text=node.text) for node in nodes]

len(documents)

documents[20].text

from llama_index.core import VectorStoreIndex

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index import VectorStoreIndex
from llama_index.core import Settings

# Set up the embedding model
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.embed_model = embed_model

# Prepare your documents
# documents = [
#     "Eiffel Tower is a wrought-iron lattice tower in Paris...",
#     "Louvre Museum is the world's largest art museum..."
# ]

# Create the VectorStoreIndex
# index = VectorStoreIndex.from_documents(documents)

db = chromadb.PersistentClient(path="./content/drive/MyDrive/verilog/database/chroma_db")
chroma_collection = db.get_or_create_collection("quickstart")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# parser = SentenceSplitter()
# nodes = parser.get_nodes_from_documents(documents)

# # build index
# index = VectorStoreIndex(nodes)

documents = [Document(text=node.text) for node in nodes if node.text.strip()]
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, storage_context=storage_context)

# Setting up the Index as Retriever
retriever = index.as_retriever()

retrieved_nodes = retriever.retrieve(" writ a 2 paragraoh description about full adder ?")

len(retrieved_nodes)

retrieved_nodes[0].metadata

retrieved_nodes[0].id_

retrieved_nodes[0].node_id

retrieved_nodes[0].node

print(retrieved_nodes[0].text)



from llama_index.core import get_response_synthesizer

hf_token="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"

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct",
    token=hf_token,
)

stopping_ids = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>"),
]

import torch
from transformers import BitsAndBytesConfig
from llama_index.core.prompts import PromptTemplate
from llama_index.llms.huggingface import HuggingFaceLLM

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)


from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B")

query_wrapper_prompt = PromptTemplate(
    template="<s> [INST] You are an expert in Hardware Description Languages (HDL), including VHDL, Verilog, and SystemVerilog. \n\n{query_str} [/INST] "
)

llm = HuggingFaceLLM(
    model_name="meta-llama/Meta-Llama-3.1-8B",
    tokenizer_name="meta-llama/Meta-Llama-3.1-8B",
    query_wrapper_prompt=PromptTemplate("<s> [INST] {query_str} [/INST] "),
    context_window=50000,
    model_kwargs={
        "token": hf_token,
        "quantization_config": quantization_config,
        "pad_token_id": tokenizer.eos_token_id,  # Add this line
    },
    tokenizer_kwargs={"token": hf_token},
    device_map="auto",
)

# llm = HuggingFaceLLM(
#     model_name="meta-llama/Llama-2-7b-chat-hf",
#     tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
#     query_wrapper_prompt=PromptTemplate("<s> [INST] {query_str} [/INST] "),
#     context_window=4096,
#     model_kwargs={"token": hf_token, "quantization_config": quantization_config},
#     tokenizer_kwargs={"token": hf_token},
#     device_map="auto",
# )

prompt=""" explain about system verilog in 1000 words"""

response = llm(prompt)

# Print the response
print(response)

response_synthesizer = get_response_synthesizer(llm=llm)

# chat_engine = index.as_chat_engine(llm=llm, response_synthesizer=response_synthesizer)

query_engine = index.as_query_engine(llm=llm, response_synthesizer=response_synthesizer)

response=query_engine.query(prompt)

# chat_engine = index.as_chat_engine(llm=llm, response_synthesizer=response_synthesizer)

# streaming_response = chat_engine.stream_chat(
#     "write full adder code in system verilog "
# )
# for token in streaming_response.response_gen:
#     print(token, end="")



response.response

response_text = response.response
lines = response_text.splitlines()

# Print with line breaks
# for line in lines:
  # print(line)

paragraphs = response_text.split("\n\n")
for paragraph in paragraphs:
  print(paragraph)
  print("\n")



response_text = response.response

sentences = response_text.split('.')

for sentence in sentences:
  print(sentence.strip() + ".")
  print("\n")

